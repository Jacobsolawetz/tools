{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55075ffe-2e3e-4e12-ac67-894eda5fe209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b79500f6-5bc1-4844-9f42-e934d206e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "from yolov5.models.experimental import attempt_load\n",
    "from yolov5.models.common import Conv\n",
    "from yolov5.models.yolo import Detect\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnxsim\n",
    "import mo.main as model_optimizer\n",
    "import subprocess\n",
    "import blobconverter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "519cbab4-b518-4681-b350-9f67700746a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \"best.pt\"\n",
    "fn = \"model\"\n",
    "f_onnx = f\"./{fn}.onnx\"\n",
    "f_simplified = f\"./{fn}-simplified.onnx\" \n",
    "dir_ov = \"./output/\"\n",
    "imgsz = 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b162a2a8-1d5a-44c9-b7b9-282399e7106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7020913 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "# based on export.py\n",
    "model = attempt_load(weights)  # load FP32 model\n",
    "nc, names = model.nc, model.names  # number of classes, class names\n",
    "\n",
    "# Checks\n",
    "opset = 12\n",
    "assert nc == len(names), f'Model class count {nc} != len(names) {len(names)}'\n",
    "\n",
    "# Input\n",
    "gs = int(max(model.stride))  # grid size (max stride)\n",
    "\n",
    "# Image size check\n",
    "if isinstance(imgsz, int):\n",
    "    imgsz = [imgsz, imgsz]\n",
    "for sz in imgsz:\n",
    "    if sz % gs != 0:\n",
    "        raise ValueError(f\"Image size is not a multiple of maximum stride {gs}\")\n",
    "\n",
    "if len(imgsz) != 2:\n",
    "    raise ValueError(f\"Image size must be of length 1 or 2.\")\n",
    "        \n",
    "im = torch.zeros(1, 3, *imgsz)#.to(device)  # image size(1,3,320,192) BCHW iDetection\n",
    "\n",
    "# Update model\n",
    "#im, model = im.half(), model.half()  # to FP16\n",
    "model.eval()\n",
    "for k, m in model.named_modules():\n",
    "    if isinstance(m, Conv):  # assign export-friendly activations\n",
    "        if isinstance(m.act, nn.SiLU):\n",
    "            m.act = SiLU()\n",
    "    elif isinstance(m, Detect):\n",
    "        m.inplace = inplace\n",
    "        m.onnx_dynamic = False\n",
    "        if hasattr(m, 'forward_export'):\n",
    "            m.forward = m.forward_export  # assign custom forward (optional)\n",
    "            \n",
    "for _ in range(2):\n",
    "    y = model(im)  # dry runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ed02eb-20bb-42ae-9015-b7000309d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ONNX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matija/Luxonis/model-export/yolo/./yolov5/models/yolo.py:57: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.onnx_dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking exported ONNX\n",
      "Simplifying\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating ONNX\")\n",
    "torch.onnx.export(model, im, f_onnx, verbose=False, opset_version=12,\n",
    "                  training=torch.onnx.TrainingMode.EVAL,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names=['images'],\n",
    "                  output_names=['output'],\n",
    "                  dynamic_axes=None)\n",
    "\n",
    "print(f\"Checking exported ONNX\")\n",
    "# Checks\n",
    "model_onnx = onnx.load(f_onnx)  # load onnx model\n",
    "onnx.checker.check_model(model_onnx)  # check onnx model\n",
    "# LOGGER.info(onnx.helper.printable_graph(model_onnx.graph))  # print\n",
    "\n",
    "print(f\"Simplifying\")\n",
    "onnx_model, check = onnxsim.simplify(model_onnx)\n",
    "assert check, 'assert check failed'\n",
    "#onnx.save(model_onnx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "816e0716-dd10-451d-bd35-206f3b35ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_indices = []\n",
    "for i, n in enumerate(onnx_model.graph.node):\n",
    "    if \"Conv\" in n.name:\n",
    "        conv_indices.append(i)\n",
    "\n",
    "input1, input2, input3 = conv_indices[-3:]\n",
    "\n",
    "sigmoid1 = onnx.helper.make_node(\n",
    "    'Sigmoid',\n",
    "    inputs=[onnx_model.graph.node[input1].output[0]],\n",
    "    outputs=['output1_yolov5'],\n",
    ")\n",
    "\n",
    "sigmoid2 = onnx.helper.make_node(\n",
    "    'Sigmoid',\n",
    "    inputs=[onnx_model.graph.node[input2].output[0]],\n",
    "    outputs=['output2_yolov5'],\n",
    ")\n",
    "\n",
    "sigmoid3 = onnx.helper.make_node(\n",
    "    'Sigmoid',\n",
    "    inputs=[onnx_model.graph.node[input3].output[0]],\n",
    "    outputs=['output3_yolov5'],\n",
    ")\n",
    "\n",
    "onnx_model.graph.node.append(sigmoid1)\n",
    "onnx_model.graph.node.append(sigmoid2)\n",
    "onnx_model.graph.node.append(sigmoid3)\n",
    "\n",
    "onnx.save(onnx_model, f_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6fe67d5-87c4-4b54-b3cb-9e6f3b5eddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_onnx = onnx.load(f_simplified)  # load onnx model\n",
    "onnx.checker.check_model(model_onnx)  # check onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3607d0-d204-43ac-ba70-32d8923c6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO replace this da pogleda≈° v export.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a3bb980-0159-405e-b369-6e608fbaa2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting export with openvino 2021.4.2-3976-0943ed67223-refs/pull/539/head...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mo --input_model ./model-simplified.onnx --output_dir ./output/ --model_name model --data_type FP16 --reverse_input_channel --scale 255 --output \"output1_yolov5,output2_yolov5,output3_yolov5\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino.inference_engine as ie\n",
    "\n",
    "print(f'Starting export with openvino {ie.__version__}...')\n",
    "\n",
    "cmd = f\"mo --input_model {f_simplified} \" \\\n",
    "f\"--output_dir {dir_ov} \" \\\n",
    "f\"--model_name {fn} \" \\\n",
    "'--data_type FP16 ' \\\n",
    "'--reverse_input_channel ' \\\n",
    "'--scale 255 ' \\\n",
    "'--output \"output1_yolov5,output2_yolov5,output3_yolov5\"'\n",
    "\n",
    "cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f03cafb-6a72-4803-b173-41613e9f13c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Const node 'Resize_118/Add_input_port_1/value214810275' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Const node 'Resize_140/Add_input_port_1/value218210278' returns shape values of 'float64' type but it must be integer or float32. During Elementwise type inference will attempt to cast to float32\n",
      "[ WARNING ]  Changing Const node 'Resize_118/Add_input_port_1/value214810512' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n",
      "[ WARNING ]  Changing Const node 'Resize_140/Add_input_port_1/value218210473' data type from float16 to <class 'numpy.float32'> for Elementwise operation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b\"Model Optimizer arguments:\\nCommon parameters:\\n\\t- Path to the Input Model: \\t/home/matija/Luxonis/model-export/yolo/./model-simplified.onnx\\n\\t- Path for generated IR: \\t/home/matija/Luxonis/model-export/yolo/./output/\\n\\t- IR output name: \\tmodel\\n\\t- Log level: \\tERROR\\n\\t- Batch: \\tNot specified, inherited from the model\\n\\t- Input layers: \\tNot specified, inherited from the model\\n\\t- Output layers: \\toutput1_yolov5,output2_yolov5,output3_yolov5\\n\\t- Input shapes: \\tNot specified, inherited from the model\\n\\t- Mean values: \\tNot specified\\n\\t- Scale values: \\tNot specified\\n\\t- Scale factor: \\t255.0\\n\\t- Precision of IR: \\tFP16\\n\\t- Enable fusing: \\tTrue\\n\\t- Enable grouped convolutions fusing: \\tTrue\\n\\t- Move mean values to preprocess section: \\tNone\\n\\t- Reverse input channels: \\tTrue\\nONNX specific parameters:\\n\\t- Inference Engine found in: \\t/home/matija/Luxonis/envs/base/lib/python3.8/site-packages/openvino\\nInference Engine version: \\t2021.4.2-3976-0943ed67223-refs/pull/539/head\\nModel Optimizer version: \\t2021.4.2-3976-0943ed67223-refs/pull/539/head\\n[ SUCCESS ] Generated IR version 10 model.\\n[ SUCCESS ] XML file: /home/matija/Luxonis/model-export/yolo/output/model.xml\\n[ SUCCESS ] BIN file: /home/matija/Luxonis/model-export/yolo/output/model.bin\\n[ SUCCESS ] Total execution time: 7.21 seconds. \\n[ SUCCESS ] Memory consumed: 164 MB. \\nIt's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "01da3680-26ae-414c-8506-e4d05ecc9adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model_openvino_2021.4_6shave.blob...\n",
      "[===========================================       ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binfile = f\"./output/{fn}.bin\"\n",
    "xmlfile = f\"./output/{fn}.xml\"\n",
    "\n",
    "blob_path = blobconverter.from_openvino(\n",
    "    xml=xmlfile,\n",
    "    bin=binfile,\n",
    "    data_type=\"FP16\",\n",
    "    shaves=6,\n",
    "    version=\"2021.4\",\n",
    "    use_cache=False,\n",
    "    output_dir=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef657288-e750-4bba-a3ef-d07550ec5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18c31f73-2540-43bf-9003-c7f90f68dedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, sides = [], []\n",
    "\n",
    "m = model.module.model[-1] if hasattr(model, 'module') else model.model[-1]\n",
    "for i in range(3):\n",
    "    sides.append(m.anchor_grid[i].size()[2])\n",
    "    for j in range(3):\n",
    "        anchors.extend(m.anchor_grid[i][0, j, 0, 0].numpy())\n",
    "        #print(np.round(m.anchor_grid[i][0, j, 0, 0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b85a2e96-5240-4da0-868a-8e87e24d28a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[76.5,\n",
       " 6.4257812,\n",
       " 23.3125,\n",
       " 25.875,\n",
       " 65.8125,\n",
       " 20.921875,\n",
       " 155.75,\n",
       " 10.5546875,\n",
       " 31.6875,\n",
       " 52.625,\n",
       " 295.5,\n",
       " 8.71875,\n",
       " 58.0625,\n",
       " 50.65625,\n",
       " 343.0,\n",
       " 19.953125,\n",
       " 168.625,\n",
       " 60.15625]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "880df00a-7624-405d-aa36-2619b8ca84fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 26, 13]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sides.sort()\n",
    "sides[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "354304a1-99c0-48a1-be46-93972b077968",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"json/yolov5.json\")\n",
    "content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a592ba2a-dbf6-49ac-92ab-cda6245fbe9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'side52': [0, 1, 2], 'side26': [3, 4, 5], 'side13': [6, 7, 8]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = dict()\n",
    "for i, num in enumerate(sides[::-1]):\n",
    "    masks[f\"side{num}\"] = list(range(i*3, i*3+3))\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6d45f63-2349-4119-b9b6-40cba3cfeea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "content[\"nn_config\"][\"input_size\"] = \"x\".join([str(x) for x in imgsz])\n",
    "content[\"nn_config\"][\"NN_specific_metadata\"][\"classes\"] = model.nc\n",
    "content[\"nn_config\"][\"NN_specific_metadata\"][\"anchors\"] = anchors\n",
    "content[\"nn_config\"][\"NN_specific_metadata\"][\"anchor_masks\"] = masks\n",
    "content[\"mappings\"][\"labels\"] = model.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18270c1-fe96-4808-a148-8b7fabc7fdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fe6cf4e-ba5e-4fa9-affa-7b4a408f42ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nn_config': {'output_format': 'detection',\n",
       "  'NN_family': 'YOLO',\n",
       "  'input_size': '416x416',\n",
       "  'NN_specific_metadata': {'classes': 4,\n",
       "   'coordinates': 4,\n",
       "   'anchors': [76.5,\n",
       "    6.4257812,\n",
       "    23.3125,\n",
       "    25.875,\n",
       "    65.8125,\n",
       "    20.921875,\n",
       "    155.75,\n",
       "    10.5546875,\n",
       "    31.6875,\n",
       "    52.625,\n",
       "    295.5,\n",
       "    8.71875,\n",
       "    58.0625,\n",
       "    50.65625,\n",
       "    343.0,\n",
       "    19.953125,\n",
       "    168.625,\n",
       "    60.15625],\n",
       "   'anchor_masks': {'side52': [0, 1, 2],\n",
       "    'side26': [3, 4, 5],\n",
       "    'side13': [6, 7, 8]},\n",
       "   'iou_threshold': 0.5,\n",
       "   'confidence_threshold': 0.5}},\n",
       " 'mappings': {'labels': ['D00', 'D01', 'D02', 'D03']}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1aaaa6-6c31-4018-8984-979fa6f24b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
